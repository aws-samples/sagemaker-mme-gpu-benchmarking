{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5a31fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark Natural Language (NLP) models using Amazon SageMaker Multi-model endpoints (MME) with GPU support\n",
    "\n",
    "Amazon SageMaker multi-model endpoints with GPU works using NVIDIA Triton Inference Server. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance. Triton supports all major training and inference frameworks, such as TensorFlow, NVIDIA TensorRT, PyTorch, MXNet, Python, ONNX, XGBoost, scikit-learn, RandomForest, OpenVINO, custom C++, and more. It offers dynamic batching, concurrent execution, post-training quantization, optimal model configuration to achieve high performance inference.\n",
    "\n",
    "In this notebook, we are going to run benchmark testing for the most popluar NLP models using MME on GPU. We will evaluate model performance such as the inference latency, throughput, and optimum model count per instance. We will also compile these models using NVIDA TensorRT to compare performance against TorchScript models.\n",
    "\n",
    "This notebook is tested on `PyTorch 1.12 Python 3.8 CPU Optimized` kernel on SageMaker Studio. An instance with at least 8 vCPU cores such as an `ml.c5.2xlarge` is recommended to run the load test. A smaller instance may be utilized by reducing the scale of the load test. The configuration provide here simulates up to 200 concurrent workers\n",
    "\n",
    "**Here is a list of model we have tested, you can use this notebook to benchmark your own models:**\n",
    "\n",
    "| Model Name      | Number of Parameters |\n",
    "| -----------     | -------------------- | \n",
    "| bert-base-uncased        | 109M                  |\n",
    "| roberta-large   | 88M                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b237a7c",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b1552a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install timm -Uqq\n",
    "%pip install transformers -Uqq\n",
    "%pip install locust -Uqq\n",
    "%pip install boto3 -Uqq\n",
    "%pip install sagemaker -Uqq\n",
    "%pip install matplotlib -Uqq\n",
    "%pip install Jinja2 -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4fc27e19-c615-4102-99b3-eb4f13b64483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc8b4725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "from utils import model_utils\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "account = sess.account_id()\n",
    "bucket = sess.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'mme-cv-benchmark'\n",
    "\n",
    "use_case = \"nlp\"\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "tested_models = [\"bert-base-uncased\", \"roberta-large\"]\n",
    "\n",
    "model_name = \"bert-base-uncased\" #change the model name to benchmark different NLP models\n",
    "\n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5503b2",
   "metadata": {},
   "source": [
    "Account Id Mapping for triton inference containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1041ca81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5abb439f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d4db3",
   "metadata": {},
   "source": [
    "## Generate Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fed9ec",
   "metadata": {},
   "source": [
    "We are going to use the following SageMaker Processing script to generate our pretrained model. This script does the following:\n",
    "\n",
    "1. Generate a model using the Pytorch Hub\n",
    "\n",
    "2. jit script the model and save the torchscript file\n",
    "\n",
    "3. Create a model artifact which is comprised of the torchscript file and a model configuration (config.pbtxt) for Triton serving\n",
    "\n",
    "Helper functions have been created for each of these steps and are imported from the `utils.model_utils` local module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87f0bad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model bert-base-uncased with 109482240 parameters\n"
     ]
    }
   ],
   "source": [
    "if model_name in tested_models:\n",
    "    tokenizer, model = model_utils.get_model_from_hf_hub(model_name)\n",
    "else:\n",
    "    warnings.warn(f\"{model_name} has not been tested and may not work\")\n",
    "    tokenizer, model = model_utils.get_model_from_hf_hub(model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"loaded model {model_name} with {model_utils.count_parameters(model)} parameters\")\n",
    "\n",
    "example_input = tokenizer(\"This is a sample\", padding=\"max_length\", max_length=max_seq_len, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976213f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packaging Pytorch model for Triton sever on SageMaker\n",
    "\n",
    "**Note**: SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`.\n",
    "\n",
    "```\n",
    "model_name\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "23b6fba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_model_path = Path(f\"triton-serve-pt/{model_name}/1\")\n",
    "pytorch_model_path.mkdir(parents=True, exist_ok=True)\n",
    "pt_model_path = model_utils.export_pt_jit(model, list(example_input.values()), pytorch_model_path) #export jit compiled model to specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dae48",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "Based on the architecture of the model we will generate a Triton configuration (config.pbtxt) file. This approach should work for most models but you may need to make adjustments to the generated config. Additionally a base model is assumed that will return the output from the last hidden state. If using a different output head such as a sequence classification, adjust the triton_outputs variable below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1eff00a-c60e-4165-9b63-91a2cda829c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get input names \n",
    "triton_inputs = [\n",
    "    {\"name\": input_name, \"data_type\": \"TYPE_INT32\", \"dims\": f\"[{max_seq_len}]\"}\n",
    "    for input_name in example_input\n",
    "]\n",
    "triton_outputs = [\n",
    "    {\n",
    "        \"name\": \"last_hidden_state\",\n",
    "        \"data_type\": \"TYPE_FP32\",\n",
    "        \"dims\": f\"[{max_seq_len}, {model.config.hidden_size}]\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bec8066a-b2a2-451c-914f-d7740d789623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_config_path = model_utils.generate_triton_config(platform=\"pt\", triton_inputs=triton_inputs,  triton_outputs=triton_outputs, save_path=pytorch_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81e9f4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll package a model config template along with the compiled model into a model.tar.gz artifact. \n",
    "# The config templates assume batch size of 32 and sequence length of 128\n",
    "# You may need to adjust the template if not using one of the tested models\n",
    "model_atifact_path = model_utils.package_triton_model(model_name, pt_model_path, triton_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf85a69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_path = f\"s3://{bucket}/{prefix}/{model_name}/\"\n",
    "initial_model_path = sess.upload_data(model_atifact_path.as_posix(), bucket=bucket, key_prefix=f\"{prefix}{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859b150",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7b99513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301217895009.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tritonserver:22.10-py3\n"
     ]
    }
   ],
   "source": [
    "from utils.endpoint_utils import create_endpoint, delete_endpoint, get_instance_utilization, run_load_test\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = f\"{account_id_map[region]}.dkr.ecr.{region}.{base}\" + \\\n",
    "            \"/sagemaker-tritonserver:22.10-py3\"\n",
    "print(mme_triton_image_uri)\n",
    "instance_type = 'ml.p3.2xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9248712a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": mme_path,\n",
    "    \"Mode\": \"MultiModel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875fa1e",
   "metadata": {},
   "source": [
    "We'll deploy and endpoint is deployed using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3dbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-west-2:376678947624:model/bert-base-uncased-pt-gpu-2022-12-27-23-30-43\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:376678947624:endpoint-config/bert-base-uncased-pt-gpu-2022-12-27-23-30-44\n",
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:376678947624:endpoint/bert-base-uncased-pt-gpu-2022-12-27-23-30-44\n",
      "Status: Creating\n",
      "Status: Creating\n"
     ]
    }
   ],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, container, instance_type, \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccd9b7",
   "metadata": {},
   "source": [
    "Next we'll upload a python model that we can use to query the instance utilization in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291029f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar czvf metrics.tar.gz server_metrics/\n",
    "!aws s3 cp metrics.tar.gz {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb93dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6999a89",
   "metadata": {},
   "source": [
    "## Load PyTorch Models into Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa7826",
   "metadata": {},
   "source": [
    "In this section we will determine the maximum number of model copies that the endpoint can load into memory within a specified threshold\n",
    "- When a model is invoked for the first time, SageMaker will load it into the GPU Memory\n",
    "- In this section we will invoke the model with a sample endpoint which result in it being loaded into memory\n",
    "- We'll then make copies of the model on S3 and invoke each copy until we reach the specified GPU Memory threshold which we set at 90% of Available memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c24fd-5c57-4c62-853d-fc7f28a3c1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": list(data.size()), \"datatype\": \"INT32\", \"data\": data.tolist()} for name, data in example_input.items()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa65813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "while True:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_model_path} {mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "        )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "    #get instance metrics\n",
    "    instance_metrics = get_instance_utilization(runtime_sm_client, endpoint_name)\n",
    "    model_avg_mem_consumption = instance_metrics[\"gpu_used_memory\"] / models_loaded\n",
    "    \n",
    "    # get an estimate of the gpu memory util once next model is loaded\n",
    "    next_gpu_mem_util = (instance_metrics[\"gpu_used_memory\"] + model_avg_mem_consumption) / instance_metrics[\"gpu_total_memory\"]\n",
    "    \n",
    "    memory_utilization = instance_metrics[\"gpu_memory_utilization\"]\n",
    "    memory_utilization_history.append(memory_utilization)\n",
    "    \n",
    "    # terminate loop if the memory consumption is exceeded once next model is loaded\n",
    "    if next_gpu_mem_util >= memory_utilization_threshold:\n",
    "        print(f\"This instance is able to load {models_loaded} models with {memory_utilization:.2%} of gpu memory consumed\")\n",
    "        break\n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4741b",
   "metadata": {},
   "source": [
    "## Benchmark Pytorch Model using Locust\n",
    "\n",
    "`locust_benchmark_sm.py` is provided in the 'locust' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541e166",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "The load test is run with up to 200 simulated workers. This may not be suitable for larger models with long response times. You can modify the <code>StagesShape</code> Class in the <code>locust/locust_benchmark_sm.py</code> file to adjust the traffic pattern and the number of concurrent workers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d72f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3de682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*pt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_load_test(endpoint_name, use_case, model_name, models_loaded, output_path, print_stdout=True, n_procs=6, sample_payload=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200954b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import some utilities to analyze the results of the load test\n",
    "from utils.viz_utils import get_summary_results, generate_summary_plots, generate_metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa859a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e34eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary = get_summary_results(locust_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb4281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_summary_plots(load_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ff0e1",
   "metadata": {},
   "source": [
    "## Clean Up PyTorch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85195772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)\n",
    "! aws s3 rm --recursive {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c187f7c",
   "metadata": {},
   "source": [
    "## Benchmark a TensorRT model\n",
    "Next we'll convert the PyTorch model to a TensorRT model with the following steps\n",
    "\n",
    "1. Convert the model to ONNX. ONNX is intemediate file format which is framework agnostic. It works with models in TF, PyTorch and more.\n",
    "\n",
    "2. Spin up a SageMaker processing job to convert ONNX model to a TensorRT model plan. You will export the weights of your model from the framework and load them into your TensorRT network.\n",
    "\n",
    "<div class=\"alert-danger\" role=\"alert\"> <strong> Warning: </strong>\n",
    "For TensorRT models, The hosting instance type must match the instance type the model is compiled on. To help ensure compatability, we'll compile the model using a SageMaker Processing job. Note that this will incur an additional cost of running the job. Also, SageMaker Processing doescurrently support ml.g5 family of instances, so these compiled models will only run on the ml.g4dn family of instances. To deploy on an ml.g5 instance, please compile the model on an EC2 or a SageMaker notebook instance \n",
    "</div>\n",
    "\n",
    "### <span style=\"color:red\"> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f582ff-81f5-4fee-8f50-aeb8a4d15f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir onnx-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0ae61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_model_path = Path(f\"onnx-models/{model_name}\")\n",
    "onnx_model_path.mkdir(exist_ok=True)\n",
    "exported_onnx_path, onnx_config = model_utils.export_onnx_nlp(model=model, tokenizer=tokenizer, save_path=onnx_model_path)\n",
    "exported_onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b45a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_compilation_args = model_utils.gen_trt_inp_compilation_config(onnx_config, max_seq_len) + [\"--fp16\",\"--verbose\"]\n",
    "trt_compilation_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a549ac3-8541-4156-bafa-95c05f0a2482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_model_path = model_utils.compile_trt(onnx_model_path=exported_onnx_path, \n",
    "                             sagemaker_session=sess, \n",
    "                             bucket=bucket,\n",
    "                             prefix=prefix,\n",
    "                             role=role,\n",
    "                             image_uri=mme_triton_image_uri, \n",
    "                             instance_type=instance_type, \n",
    "                             trt_compilation_args=trt_compilation_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51b744",
   "metadata": {},
   "source": [
    "Package TensorRT model for Triton sever on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c6b9e-80f7-4edc-bd64-e0ed37529bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_triton_config_path = model_utils.generate_triton_config(platform=\"trt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d7298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_model_atifact_path = model_utils.package_triton_model(model_name, trt_model_path, trt_triton_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_trt_model_path = sess.upload_data(trt_model_atifact_path.as_posix(), bucket=bucket, key_prefix=prefix) \n",
    "trt_mme_path = f\"s3://{bucket}/{prefix}/trt-{model_name}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eaaede",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for TensorRT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f73220",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": trt_mme_path,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, trt_container, instance_type, \"trt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec03426",
   "metadata": {},
   "source": [
    "Copy the Python model to query the instance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fab867-2d90-4746-b9ed-5c35c51e660f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp metrics.tar.gz {trt_mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab793e",
   "metadata": {},
   "source": [
    "## Load TensorRT Models into Endpoint\n",
    "We'll repeat the same procedure to determine the max number of TensorRT models that we can load in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fc5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "while True:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_trt_model_path} {trt_mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "        )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "    #get instance metrics\n",
    "    instance_metrics = get_instance_utilization(runtime_sm_client, endpoint_name)\n",
    "    model_avg_mem_consumption = instance_metrics[\"gpu_used_memory\"] / models_loaded\n",
    "    \n",
    "    # get an estimate of the gpu memory util once next model is loaded\n",
    "    next_gpu_mem_util = (instance_metrics[\"gpu_used_memory\"] + model_avg_mem_consumption) / instance_metrics[\"gpu_total_memory\"]\n",
    "    \n",
    "    memory_utilization = instance_metrics[\"gpu_memory_utilization\"]\n",
    "    memory_utilization_history.append(memory_utilization)\n",
    "    \n",
    "    # terminate loop if the memory consumption is exceeded once next model is loaded\n",
    "    if next_gpu_mem_util >= memory_utilization_threshold:\n",
    "        print(f\"This instance is able to load {models_loaded} models with {memory_utilization:.2%} of gpu memory consumed\")\n",
    "        break\n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1769104",
   "metadata": {},
   "source": [
    "## Benchmark TensorRT Model using Locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*trt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_load_test(endpoint_name, use_case, model_name, models_loaded, output_path, print_stdout=True, n_procs=6, sample_payload=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00e4f9",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f053fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test_summary = get_summary_results(locust_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e48c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary_plots(load_test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chart = generate_metrics_summary(load_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d000d4a",
   "metadata": {},
   "source": [
    "## Clean Up TensorRT Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b561919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)\n",
    "! aws s3 rm --recursive {trt_mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0351464-a625-45f7-b7b9-56d75253cb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
