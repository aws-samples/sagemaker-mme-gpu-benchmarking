{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f658fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark Computer Vision (CV) models using Amazon SageMaker Multi-model endpoints (MME) with GPU support\n",
    "\n",
    "Amazon SageMaker multi-model endpoints with GPU works using NVIDIA Triton Inference Server. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance. Triton supports all major training and inference frameworks, such as TensorFlow, NVIDIA TensorRT, PyTorch, MXNet, Python, ONNX, XGBoost, scikit-learn, RandomForest, OpenVINO, custom C++, and more. It offers dynamic batching, concurrent execution, post-training quantization, optimal model configuration to achieve high performance inference.\n",
    "\n",
    "In this notebook, we are going to run benchmark testing for the most popluar CV models using MME on GPU. We will evaluate model performance such as the inference latency, throughput, and optimum model count per instance. We will also compile these models using NVIDA TensorRT to compare performance against TorchScript models.\n",
    "\n",
    "This notebook is tested on `PyTorch 1.12 Python 3.8 CPU Optimized` kernel on SageMaker Studio. An instance with at least 8 vCPU cores such as an `ml.c5.2xlarge` is recommended to run the load test. A smaller instance may be utilized by reducing the scale of the load test. The configuration provide here simulates up to 200 concurrent workers\n",
    "\n",
    "**Here is a list of model we have tested, you can use this notebook to benchmark your own models:**\n",
    "\n",
    "| Model Name      | Number of Parameters |\n",
    "| -----------     | -------------------- | \n",
    "| resnet50        | 25M                  |\n",
    "| convNeXt Base   | 88M                  |\n",
    "| ViT Large       | 304M                 |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f5720",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b7bef78d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install timm -Uqq\n",
    "%pip install transformers -Uqq\n",
    "%pip install locust -Uqq\n",
    "%pip install boto3 -Uqq\n",
    "%pip install sagemaker -Uqq\n",
    "%pip install matplotlib -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39348ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "from utils.model_utils import get_model_from_timm, export_pt_jit, export_onnx, compile_trt, package_triton_model, count_parameters\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "account = sess.account_id()\n",
    "bucket = sess.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'mme-cv-benchmark'\n",
    "\n",
    "use_case = \"cv\"\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "tested_models = [\"resnet50\", \"vit_large_patch16_224\", \"convnext_base\"]\n",
    "model_name = \"resnet50\" #change the model name to benchmark different CV models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5ec55",
   "metadata": {},
   "source": [
    "Account Id Mapping for triton inference containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "13fdfe9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aaaa877a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce79a5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424b94f",
   "metadata": {},
   "source": [
    "We are going to use the following SageMaker Processing script to generate our pretrained model. This script does the following:\n",
    "\n",
    "1. Generate a model using the [timm](#https://huggingface.co/docs/hub/timm) library\n",
    "\n",
    "2. jit script the model and save the torchscript file\n",
    "\n",
    "3. Create a model artifact which is comprised of the torchscript file and a model configuration (config.pbtxt) for Triton serving\n",
    "\n",
    "Helper functions have been created for each of these steps and are imported from the `utils.model_utils` local module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea6fd7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model resnet50 with 25557032 parameters\n"
     ]
    }
   ],
   "source": [
    "if model_name in tested_models:\n",
    "    model = get_model_from_timm(model_name)\n",
    "    model.eval()\n",
    "else:\n",
    "    warnings.warn(f\"{model_name} has not been tested and may not work\")\n",
    "    model = get_model_from_timm(model_name)\n",
    "    model.eval()\n",
    "\n",
    "print(f\"loaded model {model_name} with {count_parameters(model)} parameters\")\n",
    "\n",
    "example_input = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e95db",
   "metadata": {},
   "source": [
    "## Packaging Pytorch model for Triton sever on SageMaker\n",
    "\n",
    "**Note**: SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`.\n",
    "\n",
    "```\n",
    "model_name\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7df063af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_model_path = Path(f\"triton-serve-pt/{model_name}/1\")\n",
    "pytorch_model_path.mkdir(parents=True, exist_ok=True)\n",
    "pt_model_path = export_pt_jit(model, example_input, pytorch_model_path) #export jit compiled model to specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc7c70",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "The example config.pbtxt templates are configured for a model that accepts a single input of shape [3,224,224] and returns a single output with the shape [-1,1000]. This should work with most image classification models trained on imagenet-1k dataset but you may need to adjust the template if testing on other models\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bec53bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll package a model config template along with the compiled model into a model.tar.gz artifact. \n",
    "# The config templates assume a 3x224x224 input with 1000 label output \n",
    "# You may need to adjust the template if not using one of the tested models\n",
    "model_atifact_path = package_triton_model(model_name, pt_model_path, \"model_config_templates/pt_cv_config.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f16af1a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_path = f\"s3://{bucket}/{prefix}/{model_name}/\"\n",
    "initial_model_path = sess.upload_data(model_atifact_path.as_posix(), bucket=bucket, key_prefix=f\"{prefix}{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654d1ae",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for PyTorch Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bb4b3ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301217895009.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tritonserver:22.10-py3\n"
     ]
    }
   ],
   "source": [
    "from utils.endpoint_utils import create_endpoint, delete_endpoint, get_instance_utilization, run_load_test\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = f\"{account_id_map[region]}.dkr.ecr.{region}.{base}\" + \\\n",
    "            \"/sagemaker-tritonserver:22.10-py3\"\n",
    "print(mme_triton_image_uri)\n",
    "instance_type = 'ml.g4dn.2xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "89ac7413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": mme_path,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": model_name},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faee2b9",
   "metadata": {},
   "source": [
    "We'll deploy and endpoint is deployed using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "feeb3fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-west-2:376678947624:model/resnet50-pt-gpu-2022-12-27-23-21-43\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:376678947624:endpoint-config/resnet50-pt-gpu-2022-12-27-23-21-43\n",
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:376678947624:endpoint/resnet50-pt-gpu-2022-12-27-23-21-43\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:376678947624:endpoint/resnet50-pt-gpu-2022-12-27-23-21-43\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, container, instance_type, \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e26ca3",
   "metadata": {},
   "source": [
    "Next we'll upload a python model that we can use to query the instance utilization in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "725cea7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server_metrics/\n",
      "server_metrics/config.pbtxt\n",
      "server_metrics/1/\n",
      "server_metrics/1/model.py\n",
      "upload: ./metrics.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/metrics.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tar czvf metrics.tar.gz server_metrics/\n",
    "!aws s3 cp metrics.tar.gz {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ee640d35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpu_utilization': 0.0,\n",
       " 'gpu_memory_utilization': 0.010920643325170428,\n",
       " 'gpu_total_memory': 15109.0,\n",
       " 'gpu_free_memory': 14944.0,\n",
       " 'gpu_used_memory': 165.0,\n",
       " 'cpu_utilization': 0.0,\n",
       " 'memory_utilization': 0.03999999910593033}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb285b7",
   "metadata": {},
   "source": [
    "## Load PyTorch Models into Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40464b",
   "metadata": {},
   "source": [
    "In this section we will determine the maximum number of model copies that the endpoint can load into memory within a specified threshold\n",
    "- When a model is invoked for the first time, SageMaker will load it into the GPU Memory\n",
    "- In this section we will invoke the model with a sample endpoint which result in it being loaded into memory\n",
    "- We'll then make copies of the model on S3 and invoke each copy until we reach the specified GPU Memory threshold which we set at 90% of Available memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7320788f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"shape\": [1, 3, 224, 224],\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": np.random.rand(3, 224,224).tolist(),\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "89263f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v0.tar.gz\n",
      "loaded 1 models with memory utilzation of 8.02%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v1.tar.gz\n",
      "loaded 2 models with memory utilzation of 9.79%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v2.tar.gz\n",
      "loaded 3 models with memory utilzation of 11.56%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v3.tar.gz\n",
      "loaded 4 models with memory utilzation of 13.32%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v4.tar.gz\n",
      "loaded 5 models with memory utilzation of 15.10%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v5.tar.gz\n",
      "loaded 6 models with memory utilzation of 17.00%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v6.tar.gz\n",
      "loaded 7 models with memory utilzation of 18.78%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v7.tar.gz\n",
      "loaded 8 models with memory utilzation of 20.56%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v8.tar.gz\n",
      "loaded 9 models with memory utilzation of 22.34%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v9.tar.gz\n",
      "loaded 10 models with memory utilzation of 24.12%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v10.tar.gz\n",
      "loaded 11 models with memory utilzation of 25.89%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v11.tar.gz\n",
      "loaded 12 models with memory utilzation of 27.79%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v12.tar.gz\n",
      "loaded 13 models with memory utilzation of 29.57%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v13.tar.gz\n",
      "loaded 14 models with memory utilzation of 31.33%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v14.tar.gz\n",
      "loaded 15 models with memory utilzation of 33.11%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v15.tar.gz\n",
      "loaded 16 models with memory utilzation of 34.90%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v16.tar.gz\n",
      "loaded 17 models with memory utilzation of 36.67%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v17.tar.gz\n",
      "loaded 18 models with memory utilzation of 38.57%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v18.tar.gz\n",
      "loaded 19 models with memory utilzation of 40.34%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v19.tar.gz\n",
      "loaded 20 models with memory utilzation of 42.11%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v20.tar.gz\n",
      "loaded 21 models with memory utilzation of 43.87%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v21.tar.gz\n",
      "loaded 22 models with memory utilzation of 45.65%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v22.tar.gz\n",
      "loaded 23 models with memory utilzation of 47.44%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v23.tar.gz\n",
      "loaded 24 models with memory utilzation of 49.35%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v24.tar.gz\n",
      "loaded 25 models with memory utilzation of 51.12%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v25.tar.gz\n",
      "loaded 26 models with memory utilzation of 52.90%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v26.tar.gz\n",
      "loaded 27 models with memory utilzation of 54.68%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v27.tar.gz\n",
      "loaded 28 models with memory utilzation of 56.44%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v28.tar.gz\n",
      "loaded 29 models with memory utilzation of 58.21%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v29.tar.gz\n",
      "loaded 30 models with memory utilzation of 60.13%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v30.tar.gz\n",
      "loaded 31 models with memory utilzation of 61.90%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v31.tar.gz\n",
      "loaded 32 models with memory utilzation of 63.68%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v32.tar.gz\n",
      "loaded 33 models with memory utilzation of 65.45%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v33.tar.gz\n",
      "loaded 34 models with memory utilzation of 67.22%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v34.tar.gz\n",
      "loaded 35 models with memory utilzation of 68.99%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v35.tar.gz\n",
      "loaded 36 models with memory utilzation of 70.89%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v36.tar.gz\n",
      "loaded 37 models with memory utilzation of 72.67%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v37.tar.gz\n",
      "loaded 38 models with memory utilzation of 74.44%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v38.tar.gz\n",
      "loaded 39 models with memory utilzation of 76.23%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v39.tar.gz\n",
      "loaded 40 models with memory utilzation of 78.00%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v40.tar.gz\n",
      "loaded 41 models with memory utilzation of 79.77%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v41.tar.gz\n",
      "loaded 42 models with memory utilzation of 81.68%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v42.tar.gz\n",
      "loaded 43 models with memory utilzation of 83.45%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v43.tar.gz\n",
      "loaded 44 models with memory utilzation of 85.23%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v44.tar.gz\n",
      "loaded 45 models with memory utilzation of 87.00%\n",
      "copy: s3://sagemaker-us-west-2-376678947624/mme-cv-benchmarkresnet50/resnet50.tar.gz to s3://sagemaker-us-west-2-376678947624/mme-cv-benchmark/resnet50/resnet50-v45.tar.gz\n",
      "This instance is able to load 46 models with 88.77% of gpu memory consumed\n"
     ]
    }
   ],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "while True:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_model_path} {mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "        )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "    #get instance metrics\n",
    "    instance_metrics = get_instance_utilization(runtime_sm_client, endpoint_name)\n",
    "    model_avg_mem_consumption = instance_metrics[\"gpu_used_memory\"] / models_loaded\n",
    "    \n",
    "    # get an estimate of the gpu memory util once next model is loaded\n",
    "    next_gpu_mem_util = (instance_metrics[\"gpu_used_memory\"] + model_avg_mem_consumption) / instance_metrics[\"gpu_total_memory\"]\n",
    "    \n",
    "    memory_utilization = instance_metrics[\"gpu_memory_utilization\"]\n",
    "    memory_utilization_history.append(memory_utilization)\n",
    "    \n",
    "    # terminate loop if the memory consumption is exceeded once next model is loaded\n",
    "    if next_gpu_mem_util >= memory_utilization_threshold:\n",
    "        print(f\"This instance is able to load {models_loaded} models with {memory_utilization:.2%} of gpu memory consumed\")\n",
    "        break\n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef0d7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark Pytorch Model using Locust\n",
    "\n",
    "`locust_benchmark_sm.py` is provided in the 'locust' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250a578-4ef0-45ff-906d-cd8a480ee6de",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "The load test is run with up to 200 simulated workers. This may not be suitable for larger models with long response times. You can modify the <code>StagesShape</code> Class in the <code>locust/locust_benchmark_sm.py</code> file to adjust the traffic pattern and the number of concurrent workers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ff9e3358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b1dca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running load test\n",
      "OrderedDict([('Type', 'InvokeEndpoint'), ('Name', 'resnet50'), ('Request Count', '156'), ('Failure Count', '0'), ('Median Response Time', '350.0'), ('Average Response Time', '683.3170586345286'), ('Min Response Time', '216.65130600013072'), ('Max Response Time', '5581.700552000257'), ('Average Content Size', '0.0'), ('Requests/s', '9.055917173865758'), ('Failures/s', '0.0'), ('50%', '370'), ('66%', '440'), ('75%', '480'), ('80%', '510'), ('90%', '740'), ('95%', '4700'), ('98%', '5600'), ('99%', '5600'), ('99.9%', '5600'), ('99.99%', '5600'), ('100%', '5600')])\n",
      "OrderedDict([('Type', 'InvokeEndpoint'), ('Name', 'resnet50'), ('Request Count', '725'), ('Failure Count', '0'), ('Median Response Time', '370.0'), ('Average Response Time', '456.84271371448204'), ('Min Response Time', '216.65130600013072'), ('Max Response Time', '5581.700552000257'), ('Average Content Size', '0.0'), ('Requests/s', '15.125603871078084'), ('Failures/s', '0.0'), ('50%', '370'), ('66%', '440'), ('75%', '470'), ('80%', '500'), ('90%', '610'), ('95%', '700'), ('98%', '820'), ('99%', '4700'), ('99.9%', '5600'), ('99.99%', '5600'), ('100%', '5600')])\n",
      "OrderedDict([('Type', 'InvokeEndpoint'), ('Name', 'resnet50'), ('Request Count', '1522'), ('Failure Count', '0'), ('Median Response Time', '450.0'), ('Average Response Time', '541.3559008061821'), ('Min Response Time', '216.65130600013072'), ('Max Response Time', '5581.700552000257'), ('Average Content Size', '0.0'), ('Requests/s', '19.94756726982949'), ('Failures/s', '0.0'), ('50%', '450'), ('66%', '560'), ('75%', '630'), ('80%', '700'), ('90%', '890'), ('95%', '1100'), ('98%', '1300'), ('99%', '1400'), ('99.9%', '5600'), ('99.99%', '5600'), ('100%', '5600')])\n",
      "OrderedDict([('Type', 'InvokeEndpoint'), ('Name', 'resnet50'), ('Request Count', '2421'), ('Failure Count', '0'), ('Median Response Time', '570.0'), ('Average Response Time', '692.6130729974983'), ('Min Response Time', '216.65130600013072'), ('Max Response Time', '5581.700552000257'), ('Average Content Size', '0.0'), ('Requests/s', '22.75008433423147'), ('Failures/s', '0.0'), ('50%', '570'), ('66%', '760'), ('75%', '910'), ('80%', '1000'), ('90%', '1200'), ('95%', '1500'), ('98%', '1700'), ('99%', '1800'), ('99.9%', '5600'), ('99.99%', '5600'), ('100%', '5600')])\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*pt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_load_test(endpoint_name, use_case, model_name, models_loaded, output_path, print_stdout=False, n_procs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7782a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import some utilities to analyze the results of the load test\n",
    "from utils.viz_utils import get_summary_results, generate_summary_plots, generate_metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194198f-d4a3-4a8b-8519-a369cb1b8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a3154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary = get_summary_results(locust_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff50134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_summary_plots(load_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e73842",
   "metadata": {},
   "source": [
    "## Clean Up PyTorch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21809f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)\n",
    "! aws s3 rm --recursive {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76efb5",
   "metadata": {},
   "source": [
    "## Benchmark a TensorRT model\n",
    "Next we'll convert the PyTorch model to a TensorRT model with the following steps\n",
    "\n",
    "1. Convert the model to ONNX. ONNX is intemediate file format which is framework agnostic. It works with models in TF, PyTorch and more.\n",
    "\n",
    "2. Spin up a SageMaker processing job to convert ONNX model to a TensorRT model plan. You will export the weights of your model from the framework and load them into your TensorRT network.\n",
    "\n",
    "<div class=\"alert-danger\" role=\"alert\"> <strong> Warning: </strong>\n",
    "For TensorRT models, The hosting instance type must match the instance type the model is compiled on. To help ensure compatability, we'll compile the model using a SageMaker Processing job. Note that this will incur an additional cost of running the job. Also, SageMaker Processing doescurrently support ml.g5 family of instances, so these compiled models will only run on the ml.g4dn family of instances. To deploy on an ml.g5 instance, please compile the model on an EC2 or a SageMaker notebook instance \n",
    "</div>\n",
    "\n",
    "### <span style=\"color:red\"> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3703df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_model_path = Path(\"onnx-models\")\n",
    "onnx_model_path.mkdir(exist_ok=True)\n",
    "exported_onnx_path = export_onnx(model=model, sample_input=example_input, save_path=onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac6b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Supply command line arguments to the trtexec command line utility\n",
    "trt_compilation_args = [\n",
    "                        \"--explicitBatch\", \n",
    "                        \"--minShapes=INPUT__0:1x3x224x224\",\n",
    "                        \"--optShapes=INPUT__0:64x3x224x224\",\n",
    "                        \"--maxShapes=INPUT__0:128x3x224x224\",\n",
    "                        \"--fp16\",\n",
    "                        \"--verbose\"\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba1ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This command could take some time to run. Up to 30 min for the models tested\n",
    "trt_model_path = compile_trt(onnx_model_path=exported_onnx_path, \n",
    "                             sagemaker_session=sess, \n",
    "                             bucket=bucket,\n",
    "                             prefix=prefix,\n",
    "                             role=role,\n",
    "                             image_uri=mme_triton_image_uri, \n",
    "                             instance_type=instance_type, \n",
    "                             trt_compilation_args=trt_compilation_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e9b10",
   "metadata": {},
   "source": [
    "Package TensorRT model for Triton sever on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fcc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_model_atifact_path = package_triton_model(model_name, trt_model_path, \"model_config_templates/trt_cv_config.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a443c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_trt_model_path = sess.upload_data(trt_model_atifact_path.as_posix(), bucket=bucket, key_prefix=prefix) \n",
    "trt_mme_path = f\"s3://{bucket}/{prefix}/trt-{model_name}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17355ce3",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for TensorRT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c2e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": trt_mme_path,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": model_name},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322dfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, trt_container, instance_type, \"trt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a723e4c",
   "metadata": {},
   "source": [
    "Copy the Python model to query the instance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a63b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp metrics.tar.gz {trt_mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ffde9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6bb54",
   "metadata": {},
   "source": [
    "## Load TensorRT Models into Endpoint\n",
    "We'll repeat the same procedure to determine the max number of TensorRT models that we can load in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e031e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "while True:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_trt_model_path} {trt_mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "        )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "    #get instance metrics\n",
    "    instance_metrics = get_instance_utilization(runtime_sm_client, endpoint_name)\n",
    "    model_avg_mem_consumption = instance_metrics[\"gpu_used_memory\"] / models_loaded\n",
    "    \n",
    "    # get an estimate of the gpu memory util once next model is loaded\n",
    "    next_gpu_mem_util = (instance_metrics[\"gpu_used_memory\"] + model_avg_mem_consumption) / instance_metrics[\"gpu_total_memory\"]\n",
    "    \n",
    "    memory_utilization = instance_metrics[\"gpu_memory_utilization\"]\n",
    "    memory_utilization_history.append(memory_utilization)\n",
    "    \n",
    "    # terminate loop if the memory consumption is exceeded once next model is loaded\n",
    "    if next_gpu_mem_util >= memory_utilization_threshold:\n",
    "        print(f\"This instance is able to load {models_loaded} models with {memory_utilization:.2%} of gpu memory consumed\")\n",
    "        break\n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9302a80",
   "metadata": {},
   "source": [
    "## Benchmark TensorRT Model using Locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833d36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2831ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*trt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_load_test(endpoint_name, use_case, model_name, models_loaded, output_path, print_stdout=False, n_procs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf04bbb",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392c5b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary = get_summary_results(locust_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d7521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945e3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_summary_plots(load_test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500061d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_chart = generate_metrics_summary(load_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2fa24e",
   "metadata": {},
   "source": [
    "## Clean Up TensorRT Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baeb4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)\n",
    "! aws s3 rm --recursive {trt_mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f71160-bd8f-4e1a-9899-55c58b7110b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5d.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
