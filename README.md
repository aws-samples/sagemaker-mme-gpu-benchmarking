## SageMaker MME Benchmarking

This repo provides a sample implementation for benchmarking Real Time Inference workloads on GPU based SageMaker Multi-Model Endpoint (MME).SageMaker [multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html) (MME) provide a scalable and cost-effective way to deploy  large numbers of ML models. It gives you the ability to deploy multiple ML models in a single serving container behind a single endpoint. From there, SageMaker will manage loading/unloading models and scaling resources on your behalf base on your traffic patterns. You will benefit from *sharing and re-using hosting resources* and a reduced *operational burden* from managing large quantity of models. 

## Usage
Make sure that your AWS identity has the requisite permissions which includes ability to create SageMaker Resources (Model, EndpointConfigs, Endpoints, and Training Jobs) in addition to S3 access to upload model artifacts. Alternatively, you can attach the [AmazonSageMakerFullAccess](https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html#security-iam-awsmanpol-AmazonSageMakerFullAccess) managed policy to your IAM User or Role.

Clone this repo into a Jupyter environment and run the provided notebooks

- [cv-benchmark.ipynb](cv-benchmark.ipynb) - Benchmark pretrained computer vision model either from torchvision or [timm](https://github.com/rwightman/pytorch-image-models) repos
- [nlp_benchmark.ipynb](nlp_benchmark.ipynb) - Benchmark pretrained NLP models from ðŸ¤— [Hugging Face Hub](https://huggingface.co/docs/hub/index)

These notebook were tested in [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/) on an **ml.c5.2xlarge** instance. An instance with 8 vCPU cores or greater is recommended for the load test.

## Additional Utilities
Additional utilities are provided within subdirectories. These may be helpful for other projects.
```
â”œâ”€â”€ locust                      Contains the load testing python script
â”‚   â””â”€â”€ locust_benchmark_sm.py
â”œâ”€â”€ model_config_templates      Contains Triton configuration templates for the various model types
â”‚   â”œâ”€â”€ pt_cv_config.pbtxt
â”‚   â”œâ”€â”€ pt_nlp_config.pbtxt
â”‚   â”œâ”€â”€ trt_cv_config.pbtxt
â”‚   â””â”€â”€ trt_nlp_config.pbtxt
â”œâ”€â”€ server_metrics              A Triton Python backend model that can be used to query endpoint metrics in real-time
â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â””â”€â”€ model.py
â”‚   â””â”€â”€ config.pbtxt
â””â”€â”€ utils                       A set of utility functions 
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ endpoint_utils.py       Endpoint configuration and deployment utils
    â”œâ”€â”€ model_utils.py          Utils for exporting models to various formats (torchscript, ONNX, TensorRT)
    â””â”€â”€ viz_utils.py            Utils for analyzing and visualizing results
```

## Benchmark Outputs
When running the notebooks, results of the benchmark will be automatically written into a new `results` directory. The directory has the following structure where the top level folder is the name of the model benchmarked (e.g. bert-base-uncased) with several csv files that represent the raw [Locust output](https://docs.locust.io/en/stable/retrieving-stats.html) with the following naming convention **<instance_type>*<platform>*<model_loaded>_<locust_output_type>.csv**. The `summary_results.csv` file provides the overall summary of the benchmark.
```
â”œâ”€â”€ results
â”‚   â”œâ”€â”€ bert-base-uncased
â”‚   â”‚   â”œâ”€â”€ ml.g4dn.2xlarge*pt*26_exceptions.csv
â”‚   â”‚   â”œâ”€â”€ ml.g4dn.2xlarge*pt*26_failures.csv
â”‚   â”‚   â”œâ”€â”€ ml.g4dn.2xlarge*pt*26_stats.csv
â”‚   â”‚   â”œâ”€â”€ ml.g4dn.2xlarge*pt*26_stats_history.csv
â”‚   â”‚   â”œâ”€â”€ ml.g4dn.2xlarge*trt*42_exceptions.csv
â”‚   â”‚   â”œâ”€â”€ ml.g4dn.2xlarge*trt*42_failures.csv
â”‚   â”‚   â”œâ”€â”€ ml.g4dn.2xlarge*trt*42_stats.csv
â”‚   â”‚   â”œâ”€â”€ ml.g4dn.2xlarge*trt*42_stats_history.csv
â”‚   â”‚   â””â”€â”€ summary_results.csv
```
In addition to the raw outputs, the notebook generates a pair of charts to provide a visual summary of the results.
The first chart provides a line graph that plots latency and throughput metrics (y-axis) for increasing number of concurrent users (x-axis) and a bar graph that shows the count of successful and failed inference requests.

<img src="images/chart1.png" width="800"/>

The second chart shows a comparison of metrics between the PyTorch (libtorch) and TensorRT backends including the number of models loaded, the lowest latency, highest throughput, and the max number of concurrent users without any failed requests.

<img src="images/chart2.png" width="800"/>

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.

