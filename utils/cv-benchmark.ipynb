{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f658fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark Computer Vision (CV) models using Amazon SageMaker Multi-model endpoints (MME) with GPU support\n",
    "\n",
    "Amazon SageMaker multi-model endpoints with GPU works using NVIDIA Triton Inference Server. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance. Triton supports all major training and inference frameworks, such as TensorFlow, NVIDIA TensorRT, PyTorch, MXNet, Python, ONNX, XGBoost, scikit-learn, RandomForest, OpenVINO, custom C++, and more. It offers dynamic batching, concurrent execution, post-training quantization, optimal model configuration to achieve high performance inference.\n",
    "\n",
    "In this notebook, we are going to run benchmark testing for the most popluar CV models using MME on GPU. We will evaluate model performance such as the inference latency, throughput, and optimum model count per instance. We will also compile these models using NVIDA TensorRT to compare performance against TorchScript models.\n",
    "\n",
    "This notebook is tested on `PyTorch 1.12 Python 3.8 CPU Optimized` kernel on SageMaker Studio. An instance with at least 8 vCPU cores such as an `ml.c5.2xlarge` is recommended to run the load test. A smaller instance may be utilized by reducing the scale of the load test. The configuration provide here simulates up to 200 concurrent workers\n",
    "\n",
    "**Here is a list of model we have tested, you can use this notebook to benchmark your own models:**\n",
    "\n",
    "| Model Name      | Number of Parameters |\n",
    "| -----------     | -------------------- | \n",
    "| resnet50        | 25M                  |\n",
    "| convNeXt Base   | 88M                  |\n",
    "| ViT Large       | 304M                 |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f5720",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bef78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install timm -Uqq\n",
    "%pip install locust -Uqq\n",
    "%pip install boto3 -Uqq\n",
    "%pip install sagemaker -Uqq\n",
    "%pip install matplotlib -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39348ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "from utils.model_utils import get_model_from_timm, export_pt_jit, export_onnx, compile_trt, package_triton_model, count_parameters\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "account = sess.account_id()\n",
    "bucket = sess.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'mme-cv-benchmark'\n",
    "\n",
    "use_case = \"cv\"\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "tested_models = [\"resnet50\", \"vit_large_patch16_224\", \"convnext_base\"]\n",
    "model_name = \"resnet50\" #change the model name to benchmark different CV models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5ec55",
   "metadata": {},
   "source": [
    "Account Id Mapping for triton inference containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdfe9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa877a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce79a5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424b94f",
   "metadata": {},
   "source": [
    "We are going to use the following SageMaker Processing script to generate our pretrained model. This script does the following:\n",
    "\n",
    "1. Generate a model using the [timm](#https://huggingface.co/docs/hub/timm) library\n",
    "\n",
    "2. jit script the model and save the torchscript file\n",
    "\n",
    "3. Create a model artifact which is comprised of the torchscript file and a model configuration (config.pbtxt) for Triton serving\n",
    "\n",
    "Helper functions have been created for each of these steps and are imported from the `utils.model_utils` local module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fd7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_name in tested_models:\n",
    "    model = get_model_from_timm(model_name)\n",
    "    model.eval()\n",
    "else:\n",
    "    warnings.warn(f\"{model_name} has not been tested and may not work\")\n",
    "    model = get_model_from_timm(model_name)\n",
    "    model.eval()\n",
    "\n",
    "print(f\"loaded model {model_name} with {count_parameters(model)} parameters\")\n",
    "\n",
    "example_input = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e95db",
   "metadata": {},
   "source": [
    "## Packaging Pytorch model for Triton sever on SageMaker\n",
    "\n",
    "**Note**: SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`.\n",
    "\n",
    "```\n",
    "model_name\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df063af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_model_path = Path(f\"triton-serve-pt/{model_name}/1\")\n",
    "pytorch_model_path.mkdir(parents=True, exist_ok=True)\n",
    "pt_model_path = export_pt_jit(model, example_input, pytorch_model_path) #export jit compiled model to specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc7c70",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "The example config.pbtxt templates are configured for a model that accepts a single input of shape [3,224,224] and returns a single output with the shape [-1,1000]. This should work with most image classification models trained on imagenet-1k dataset but you may need to adjust the template if testing on other models\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec53bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll package a model config template along with the compiled model into a model.tar.gz artifact. \n",
    "# The config templates assume a 3x224x224 input with 1000 label output \n",
    "# You may need to adjust the template if not using one of the tested models\n",
    "model_atifact_path = package_triton_model(model_name, pt_model_path, \"model_config_templates/pt_cv_config.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16af1a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_path = f\"s3://{bucket}/{prefix}/{model_name}/\"\n",
    "initial_model_path = sess.upload_data(model_atifact_path.as_posix(), bucket=bucket, key_prefix=f\"{prefix}{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654d1ae",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for PyTorch Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b3ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.endpoint_utils import create_endpoint, delete_endpoint, get_instance_utilization, run_load_test\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = f\"{account_id_map[region]}.dkr.ecr.{region}.{base}\" + \\\n",
    "            \"/sagemaker-tritonserver:22.07-py3\"\n",
    "print(mme_triton_image_uri)\n",
    "instance_type = 'ml.g4dn.2xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac7413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": mme_path,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": model_name},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faee2b9",
   "metadata": {},
   "source": [
    "We'll deploy and endpoint is deployed using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb3fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, container, instance_type, \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e26ca3",
   "metadata": {},
   "source": [
    "Next we'll upload a python model that we can use to query the instance utilization in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cea7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar czvf metrics.tar.gz server_metrics/\n",
    "!aws s3 cp metrics.tar.gz {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee640d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb285b7",
   "metadata": {},
   "source": [
    "## Load PyTorch Models into Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40464b",
   "metadata": {},
   "source": [
    "In this section we will determine the maximum number of model copies that the endpoint can load into memory within a specified threshold\n",
    "- When a model is invoked for the first time, SageMaker will load it into the GPU Memory\n",
    "- In this section we will invoke the model with a sample endpoint which result in it being loaded into memory\n",
    "- We'll then make copies of the model on S3 and invoke each copy until we reach the specified GPU Memory threshold which we set at 90% of Available memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320788f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"shape\": [1, 3, 224, 224],\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": np.random.rand(3, 224,224).tolist(),\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89263f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "while True:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_model_path} {mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "        )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "    #get instance metrics\n",
    "    instance_metrics = get_instance_utilization(runtime_sm_client, endpoint_name)\n",
    "    model_avg_mem_consumption = instance_metrics[\"gpu_used_memory\"] / models_loaded\n",
    "    \n",
    "    # get an estimate of the gpu memory util once next model is loaded\n",
    "    next_gpu_mem_util = (instance_metrics[\"gpu_used_memory\"] + model_avg_mem_consumption) / instance_metrics[\"gpu_total_memory\"]\n",
    "    \n",
    "    memory_utilization = instance_metrics[\"gpu_memory_utilization\"]\n",
    "    memory_utilization_history.append(memory_utilization)\n",
    "    \n",
    "    # terminate loop if the memory consumption is exceeded once next model is loaded\n",
    "    if next_gpu_mem_util >= memory_utilization_threshold:\n",
    "        print(f\"This instance is able to load {models_loaded} models with {memory_utilization:.2%} of gpu memory consumed\")\n",
    "        break\n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef0d7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark Pytorch Model using Locust\n",
    "\n",
    "`locust_benchmark_sm.py` is provided in the 'locust' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250a578-4ef0-45ff-906d-cd8a480ee6de",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "The load test is run with up to 200 simulated workers. This may not be suitable for larger models with long response times. You can modify the <code>StagesShape</code> Class in the <code>locust/locust_benchmark_sm.py</code> file to adjust the traffic pattern and the number of concurrent workers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e3358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b1dca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*pt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_load_test(endpoint_name, model_name, models_loaded, output_path, print_stdout=False, n_procs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7782a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import some utilities to analyze the results of the load test\n",
    "from utils.viz_utils import get_summary_results, generate_summary_plots, generate_metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194198f-d4a3-4a8b-8519-a369cb1b8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a3154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary = get_summary_results(locust_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff50134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_summary_plots(load_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e73842",
   "metadata": {},
   "source": [
    "## Clean Up PyTorch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21809f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)\n",
    "! aws s3 rm --recursive {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76efb5",
   "metadata": {},
   "source": [
    "## Benchmark a TensorRT model\n",
    "Next we'll convert the PyTorch model to a TensorRT model with the following steps\n",
    "\n",
    "1. Convert the model to ONNX. ONNX is intemediate file format which is framework agnostic. It works with models in TF, PyTorch and more.\n",
    "\n",
    "2. Spin up a SageMaker processing job to convert ONNX model to a TensorRT model plan. You will export the weights of your model from the framework and load them into your TensorRT network.\n",
    "\n",
    "<div class=\"alert-danger\" role=\"alert\"> <strong> Warning: </strong>\n",
    "For TensorRT models, The hosting instance type must match the instance type the model is compiled on. To help ensure compatability, we'll compile the model using a SageMaker Processing job. Note that this will incur an additional cost of running the job. Also, SageMaker Processing doescurrently support ml.g5 family of instances, so these compiled models will only run on the ml.g4dn family of instances. To deploy on an ml.g5 instance, please compile the model on an EC2 or a SageMaker notebook instance \n",
    "</div>\n",
    "\n",
    "### <span style=\"color:red\"> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3703df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_model_path = Path(\"onnx-models\")\n",
    "onnx_model_path.mkdir(exist_ok=True)\n",
    "exported_onnx_path = export_onnx(model=model, sample_input=example_input, save_path=onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac6b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Supply command line arguments to the trtexec command line utility\n",
    "trt_compilation_args = [\n",
    "                        \"--explicitBatch\", \n",
    "                        \"--minShapes=INPUT__0:1x3x224x224\",\n",
    "                        \"--optShapes=INPUT__0:64x3x224x224\",\n",
    "                        \"--maxShapes=INPUT__0:128x3x224x224\",\n",
    "                        \"--fp16\",\n",
    "                        \"--verbose\"\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba1ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This command could take some time to run. Up to 30 min for the models tested\n",
    "trt_model_path = compile_trt(onnx_model_path=exported_onnx_path, \n",
    "                             sagemaker_session=sess, \n",
    "                             bucket=bucket,\n",
    "                             prefix=prefix,\n",
    "                             role=role,\n",
    "                             image_uri=mme_triton_image_uri, \n",
    "                             instance_type=instance_type, \n",
    "                             trt_compilation_args=trt_compilation_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e9b10",
   "metadata": {},
   "source": [
    "Package TensorRT model for Triton sever on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fcc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_model_atifact_path = package_triton_model(model_name, trt_model_path, \"model_config_templates/trt_cv_config.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a443c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_trt_model_path = sess.upload_data(trt_model_atifact_path.as_posix(), bucket=bucket, key_prefix=prefix) \n",
    "trt_mme_path = f\"s3://{bucket}/{prefix}/trt-{model_name}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17355ce3",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for TensorRT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c2e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trt_container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": trt_mme_path,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": model_name},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322dfc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, trt_container, instance_type, \"trt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a723e4c",
   "metadata": {},
   "source": [
    "Copy the Python model to query the instance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a63b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp metrics.tar.gz {trt_mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ffde9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6bb54",
   "metadata": {},
   "source": [
    "## Load TensorRT Models into Endpoint\n",
    "We'll repeat the same procedure to determine the max number of TensorRT models that we can load in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e031e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "while True:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_trt_model_path} {trt_mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "        )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "    #get instance metrics\n",
    "    instance_metrics = get_instance_utilization(runtime_sm_client, endpoint_name)\n",
    "    model_avg_mem_consumption = instance_metrics[\"gpu_used_memory\"] / models_loaded\n",
    "    \n",
    "    # get an estimate of the gpu memory util once next model is loaded\n",
    "    next_gpu_mem_util = (instance_metrics[\"gpu_used_memory\"] + model_avg_mem_consumption) / instance_metrics[\"gpu_total_memory\"]\n",
    "    \n",
    "    memory_utilization = instance_metrics[\"gpu_memory_utilization\"]\n",
    "    memory_utilization_history.append(memory_utilization)\n",
    "    \n",
    "    # terminate loop if the memory consumption is exceeded once next model is loaded\n",
    "    if next_gpu_mem_util >= memory_utilization_threshold:\n",
    "        print(f\"This instance is able to load {models_loaded} models with {memory_utilization:.2%} of gpu memory consumed\")\n",
    "        break\n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9302a80",
   "metadata": {},
   "source": [
    "## Benchmark TensorRT Model using Locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833d36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2831ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*trt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_load_test(endpoint_name, model_name, models_loaded, output_path, print_stdout=False, n_procs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf04bbb",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392c5b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary = get_summary_results(locust_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d7521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945e3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_summary_plots(load_test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500061d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_chart = generate_metrics_summary(load_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2fa24e",
   "metadata": {},
   "source": [
    "## Clean Up TensorRT Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baeb4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)\n",
    "! aws s3 rm --recursive {trt_mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f71160-bd8f-4e1a-9899-55c58b7110b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
